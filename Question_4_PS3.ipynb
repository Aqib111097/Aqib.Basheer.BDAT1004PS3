{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8023a8c-e9a9-4d4d-9e42-6893b8af3c19",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 2 fields in line 9, saw 6\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Step 2: Import the dataset\u001b[39;00m\n\u001b[0;32m      5\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://localhost:8888/edit/wind.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 6\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(url, delim_whitespace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Step 3: Assign proper datetime index\u001b[39;00m\n\u001b[0;32m      9\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(data[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYr\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMo\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDy\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:617\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\u001b[38;5;241m.\u001b[39mread(nrows)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1748\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1741\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1743\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m     (\n\u001b[0;32m   1745\u001b[0m         index,\n\u001b[0;32m   1746\u001b[0m         columns,\n\u001b[0;32m   1747\u001b[0m         col_dict,\n\u001b[1;32m-> 1748\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mread(  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m   1749\u001b[0m         nrows\n\u001b[0;32m   1750\u001b[0m     )\n\u001b[0;32m   1751\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1752\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mread_low_memory(nrows)\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mparsers.pyx:843\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:904\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:879\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:890\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2058\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 2 fields in line 9, saw 6\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Step 2: Import the dataset\n",
    "url = \"http://localhost:8888/edit/wind.txt\"\n",
    "data = pd.read_csv(url, delim_whitespace=True)\n",
    "\n",
    "# Step 3: Assign proper datetime index\n",
    "data['date'] = pd.to_datetime(data[['Yr', 'Mo', 'Dy']])\n",
    "data = data.set_index('date')\n",
    "data = data.drop(columns=['Yr', 'Mo', 'Dy'])\n",
    "\n",
    "# Step 4: Fix year 2061\n",
    "def fix_year(year):\n",
    "    if year > 2000:\n",
    "        year -= 100\n",
    "    return year\n",
    "\n",
    "data.index = data.index.map(lambda x: x.replace(year=fix_year(x.year)))\n",
    "\n",
    "# Step 5: Set the right dates as the index\n",
    "data.index = pd.to_datetime(data.index)\n",
    "\n",
    "# Step 6: Compute missing values per location\n",
    "missing_values_per_location = data.isnull().sum()\n",
    "\n",
    "# Step 7: Compute non-missing values in total\n",
    "total_non_missing_values = data.notnull().sum().sum()\n",
    "\n",
    "# Step 8: Calculate mean windspeeds\n",
    "mean_windspeed = data.stack().mean()\n",
    "\n",
    "# Step 9: Calculate min, max, mean, and standard deviation windspeeds per location\n",
    "loc_stats = data.describe().transpose()\n",
    "\n",
    "# Step 10: Calculate min, max, mean, and standard deviation windspeeds per day\n",
    "day_stats = data.resample('D').agg(['min', 'max', 'mean', 'std'])\n",
    "\n",
    "# Step 11: Average windspeed in January for each location\n",
    "january_average = data[data.index.month == 1].groupby(data.index.year).mean()\n",
    "\n",
    "# Step 12: Downsample to yearly frequency\n",
    "yearly_data = data.resample('Y').mean()\n",
    "\n",
    "# Step 13: Downsample to monthly frequency\n",
    "monthly_data = data.resample('M').mean()\n",
    "\n",
    "# Step 14: Downsample to weekly frequency\n",
    "weekly_data = data.resample('W').mean()\n",
    "\n",
    "# Step 15: Calculate min, max, mean, and standard deviation windspeeds per week for the first 52 weeks\n",
    "weekly_stats = data.resample('W').agg(['min', 'max', 'mean', 'std']).iloc[:52]\n",
    "\n",
    "# Displaying the results\n",
    "print(\"Step 6: Missing values per location:\\n\", missing_values_per_location)\n",
    "print(\"\\nStep 7: Total non-missing values:\", total_non_missing_values)\n",
    "print(\"\\nStep 8: Mean windspeed over all locations and times:\", mean_windspeed)\n",
    "print(\"\\nStep 9: Statistics per location:\\n\", loc_stats)\n",
    "print(\"\\nStep 10: Statistics per day:\\n\", day_stats)\n",
    "print(\"\\nStep 11: Average windspeed in January for each location:\\n\", january_average)\n",
    "print(\"\\nStep 12: Yearly frequency:\\n\", yearly_data)\n",
    "print(\"\\nStep 13: Monthly frequency:\\n\", monthly_data)\n",
    "print(\"\\nStep 14: Weekly frequency:\\n\", weekly_data)\n",
    "print(\"\\nStep 15: Weekly statistics for the first 52 weeks:\\n\", weekly_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fbaef8-6ee2-4f73-8772-9c44046eb83b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
